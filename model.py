# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xqg3TJrO2ANOFULOjg2ILdE6d5PdCBtr
"""

# Bank Marketing - Machine Learning Classification Project

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve
import optuna
from imblearn.over_sampling import SMOTE


# 1. Load Data
df = pd.read_csv("bank-additional.csv", sep=';')


# Veri boyutu ve ilk 5 satÄ±r
print(df.shape)
df.head()

pip install streamlit

# 2. Clean 'unknown' values
columns_to_fill = ['job', 'marital', 'education', 'housing', 'loan']
for column in columns_to_fill:
    df[column] = df[column].replace('unknown', df[column].mode()[0])

# 3. Preprocessing
X = df.drop(columns='y')
y = df['y']

# 3.a Feature Engineering
X['campaign_per_previous'] = X['campaign'] / (X['previous'] + 1)
X['contact_month_combo'] = X['contact'] + '_' + X['month']
X['loan_and_housing'] = X['loan'] + '_' + X['housing']

# Re-encode new categorical columns
categorical_cols = X.select_dtypes(include='object').columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Scale numerical columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Encode target
y_encoded = LabelEncoder().fit_transform(y)

# 4. Feature Importance (Optional)
rf = RandomForestClassifier(random_state=42)
rf.fit(X, y_encoded)
importances = rf.feature_importances_
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.gca().invert_yaxis()
plt.title('Feature Importance (Random Forest)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()

# 5. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# 6. Apply SMOTE for class balance
sm = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)

# 7. Model Evaluation with SMOTE data
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Neural Network": MLPClassifier(max_iter=300, random_state=42),
    "Support Vector Machine": SVC(probability=True),
    "Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

results = []
best_model = None
best_name = None
best_f1 = 0

for name, model in models.items():
    model.fit(X_train_balanced, y_train_balanced)
    y_pred = model.predict(X_test)
    f1 = f1_score(y_test, y_pred)
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1
    })
    if f1 > best_f1:
        best_f1 = f1
        best_model = model
        best_name = name

# 7.a Stacking Classifier
stack_model = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(random_state=42)),
        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),
        ('svm', SVC(probability=True))
    ],
    final_estimator=LogisticRegression()
)
stack_model.fit(X_train_balanced, y_train_balanced)
y_pred_stack = stack_model.predict(X_test)
stack_f1 = f1_score(y_test, y_pred_stack)
results.append({
    "Model": "Stacking Classifier",
    "Accuracy": accuracy_score(y_test, y_pred_stack),
    "Precision": precision_score(y_test, y_pred_stack),
    "Recall": recall_score(y_test, y_pred_stack),
    "F1 Score": stack_f1
})
if stack_f1 > best_f1:
    best_f1 = stack_f1
    best_model = stack_model
    best_name = "Stacking Classifier"

# 7.b Threshold Optimization for Best Model
probs = best_model.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, probs)
f1s = 2 * (precisions * recalls) / (precisions + recalls)
best_threshold = thresholds[np.argmax(f1s)]
y_pred_opt = (probs > best_threshold).astype(int)

results.append({
    "Model": f"{best_name} (Threshold Optimized)",
    "Accuracy": accuracy_score(y_test, y_pred_opt),
    "Precision": precision_score(y_test, y_pred_opt),
    "Recall": recall_score(y_test, y_pred_opt),
    "F1 Score": f1_score(y_test, y_pred_opt)
})

results_df = pd.DataFrame(results).sort_values(by="F1 Score", ascending=False)
print(results_df)

# 9. Train Final XGBoost Model with Best Parameters from Previous Optuna Search
xgb_final_params = {
    'n_estimators': 300,
    'max_depth': 10,
    'learning_rate': 0.07216874784717071,
    'subsample': 0.988839400807138,
    'colsample_bytree': 0.8801296358249565,
    'gamma': 3.532791449428222,
    'use_label_encoder': False,
    'eval_metric': 'logloss'
}

final_model = XGBClassifier(**xgb_final_params)
final_model.fit(X_train_balanced, y_train_balanced)
probs_final = final_model.predict_proba(X_test)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_test, probs_final)
f1s = 2 * (precisions * recalls) / (precisions + recalls)
best_threshold = thresholds[np.argmax(f1s)]
y_pred_final = (probs_final > best_threshold).astype(int)

print("\nðŸ“Œ Final Model: XGBoost (Optuna + Threshold Optimized)")
print("Best Threshold:", best_threshold)
print("Accuracy:", accuracy_score(y_test, y_pred_final))
print("Precision:", precision_score(y_test, y_pred_final))
print("Recall:", recall_score(y_test, y_pred_final))
print("F1 Score:", f1_score(y_test, y_pred_final))

# STREAMLIT UI
st.title("Term Deposit Subscription Prediction")
st.write("Enter client information to predict if they will subscribe to a term deposit:")

input_data = {}
for col in X.columns:
    if col in numerical_cols:
        input_data[col] = st.number_input(f"{col}", value=0.0)
    else:
        input_data[col] = st.selectbox(f"{col}", options=list(df[col].unique()))

if st.button("Predict"):
    input_df = pd.DataFrame([input_data])
    for col in categorical_cols:
        input_df[col] = label_encoders[col].transform(input_df[col])
    input_df[numerical_cols] = scaler.transform(input_df[numerical_cols])
    proba = final_model.predict_proba(input_df)[:, 1]
    prediction = (proba > best_threshold).astype(int)[0]

    st.write("\n### Result:")
    if prediction == 1:
        st.success("Client is likely to subscribe to a term deposit.")
    else:
        st.warning("Client is unlikely to subscribe to a term deposit.")